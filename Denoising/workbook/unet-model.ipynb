{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12320532,"sourceType":"datasetVersion","datasetId":7766005},{"sourceId":12469016,"sourceType":"datasetVersion","datasetId":7866451}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport re\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T09:40:42.388592Z","iopub.execute_input":"2025-07-14T09:40:42.388807Z","iopub.status.idle":"2025-07-14T09:40:53.331161Z","shell.execute_reply.started":"2025-07-14T09:40:42.388784Z","shell.execute_reply":"2025-07-14T09:40:53.330605Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class PositionalEncoding2D(nn.Module):\n    def __init__(self, embed_dim: int):\n        super().__init__()\n        self.proj = nn.Conv2d(2, embed_dim, kernel_size=1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, C, H, W = x.shape\n        device = x.device\n\n        y_lin = torch.linspace(-1.0, 1.0, steps=H, device=device)\n        x_lin = torch.linspace(-1.0, 1.0, steps=W, device=device)\n        y_grid = y_lin.view(1, 1, H, 1).expand(B, 1, H, W)\n        x_grid = x_lin.view(1, 1, 1, W).expand(B, 1, H, W)\n        coords = torch.cat([x_grid, y_grid], dim=1)\n\n        pos_emb = self.proj(coords)\n        return x + pos_emb\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UpConv(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n\n    def forward(self, x):\n        return self.up(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, F_g: int, F_l: int, F_int: int):\n        super().__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int),\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int),\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x: torch.Tensor, g: torch.Tensor) -> torch.Tensor:\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        mask = self.psi(psi)\n        return x * mask\n\nclass AttentionUNet(nn.Module):\n    def __init__(\n        self,\n        in_channels: int = 3,\n        out_channels: int = 3,\n        enc_channels: tuple = (128, 64, 32, 16),\n    ):\n        super().__init__()\n        assert len(enc_channels) == 4\n\n        self.in_conv = ConvBlock(in_channels, enc_channels[0])\n        self.pos0 = PositionalEncoding2D(enc_channels[0])\n        self.pool = nn.MaxPool2d(2)\n\n        self.enc1 = ConvBlock(enc_channels[0], enc_channels[1])\n        self.pos1 = PositionalEncoding2D(enc_channels[1])\n\n        self.enc2 = ConvBlock(enc_channels[1], enc_channels[2])\n        self.pos2 = PositionalEncoding2D(enc_channels[2])\n\n        self.enc3 = ConvBlock(enc_channels[2], enc_channels[3])\n        self.pos3 = PositionalEncoding2D(enc_channels[3])\n\n        self.up_stage3 = UpConv(enc_channels[3], enc_channels[2])\n        self.att_stage3 = AttentionBlock(\n            F_g=enc_channels[2], F_l=enc_channels[2], F_int=enc_channels[2] // 2\n        )\n        self.conv_stage3 = ConvBlock(enc_channels[2] + enc_channels[2], enc_channels[2])\n        self.pos_d3 = PositionalEncoding2D(enc_channels[2])\n\n        self.up_stage2 = UpConv(enc_channels[2], enc_channels[1])\n        self.att_stage2 = AttentionBlock(\n            F_g=enc_channels[1], F_l=enc_channels[1], F_int=enc_channels[1] // 2\n        )\n        self.conv_stage2 = ConvBlock(enc_channels[1] + enc_channels[1], enc_channels[1])\n        self.pos_d2 = PositionalEncoding2D(enc_channels[1])\n\n        self.up_stage1 = UpConv(enc_channels[1], enc_channels[0])\n        self.att_stage1 = AttentionBlock(\n            F_g=enc_channels[0], F_l=enc_channels[0], F_int=enc_channels[0] // 2\n        )\n        self.conv_stage1 = ConvBlock(enc_channels[0] + enc_channels[0], enc_channels[0])\n        self.pos_d1 = PositionalEncoding2D(enc_channels[0])\n\n        self.out_conv = nn.Sequential(\n            nn.Conv2d(enc_channels[0], out_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x0 = self.pos0(self.in_conv(x))\n        x1 = self.pos1(self.enc1(self.pool(x0)))\n        x2 = self.pos2(self.enc2(self.pool(x1)))\n        x3 = self.pos3(self.enc3(self.pool(x2)))\n\n        d3_up = self.up_stage3(x3)\n        x2_att = self.att_stage3(x2, d3_up)\n        d3_concat = torch.cat([d3_up, x2_att], dim=1)\n        d3 = self.pos_d3(self.conv_stage3(d3_concat))\n\n        d2_up = self.up_stage2(d3)\n        x1_att = self.att_stage2(x1, d2_up)\n        d2_concat = torch.cat([d2_up, x1_att], dim=1)\n        d2 = self.pos_d2(self.conv_stage2(d2_concat))\n\n        d1_up = self.up_stage1(d2)\n        x0_att = self.att_stage1(x0, d1_up)\n        d1_concat = torch.cat([d1_up, x0_att], dim=1)\n        d1 = self.pos_d1(self.conv_stage1(d1_concat))\n\n        return self.out_conv(d1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T09:41:12.378016Z","iopub.execute_input":"2025-07-14T09:41:12.378789Z","iopub.status.idle":"2025-07-14T09:41:12.401750Z","shell.execute_reply.started":"2025-07-14T09:41:12.378757Z","shell.execute_reply":"2025-07-14T09:41:12.400890Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class AugraphyPatchDataset(Dataset):\n    def __init__(self, input_dir, aug_dirs, patch_size=(256, 256), overlap=96, transform=None, num_images_to_use=None):\n        self.patch_w, self.patch_h = patch_size\n        self.overlap = overlap\n        self.stride_w = self.patch_w - self.overlap\n        self.stride_h = self.patch_h - self.overlap\n        if self.stride_w <= 0 or self.stride_h <= 0:\n            raise ValueError(\"Stride must be positive. Patch size must be greater than overlap.\")\n\n        self.transform = transform if transform else transforms.ToTensor()\n\n        self.data_pairs = []\n        self.clean_image_map = {\n            os.path.splitext(f)[0].lower(): os.path.join(input_dir, f)\n            for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))\n        }\n\n        all_aug_paths = []\n        for aug_dir in aug_dirs:\n            for aug_file_name in os.listdir(aug_dir):\n                if aug_file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n                    aug_img_path = os.path.join(aug_dir, aug_file_name)\n                    aug_base_name = os.path.splitext(aug_file_name)[0].lower()\n\n                    best_match_parent = None\n                    for clean_base in self.clean_image_map.keys():\n                        if aug_base_name.startswith(clean_base) and (\n                            len(aug_base_name) == len(clean_base) or\n                            (len(aug_base_name) > len(clean_base) and aug_base_name[len(clean_base)] == '_')\n                        ):\n                            if best_match_parent is None or len(clean_base) > len(best_match_parent):\n                                best_match_parent = clean_base\n\n                    if best_match_parent and best_match_parent in self.clean_image_map:\n                        clean_img_path = self.clean_image_map[best_match_parent]\n                        all_aug_paths.append((aug_img_path, clean_img_path))\n\n        if num_images_to_use is not None and num_images_to_use > 0:\n            self.data_pairs = all_aug_paths[:num_images_to_use]\n        else:\n            self.data_pairs = all_aug_paths\n\n        self.patch_coords = []\n        for img_idx, (aug_img_path, _) in enumerate(self.data_pairs):\n            with Image.open(aug_img_path) as img:\n                img_width, img_height = img.size\n\n            num_patches_x = math.ceil((img_width - self.patch_w + self.stride_w) / self.stride_w) if img_width > self.patch_w else 1\n            num_patches_y = math.ceil((img_height - self.patch_h + self.stride_h) / self.stride_h) if img_height > self.patch_h else 1\n\n            if img_width <= self.patch_w: num_patches_x = 1\n            if img_height <= self.patch_h: num_patches_y = 1\n\n            for r_idx in range(num_patches_y):\n                for c_idx in range(num_patches_x):\n                    x_start = c_idx * self.stride_w\n                    y_start = r_idx * self.stride_h\n\n                    x_start_actual = min(x_start, img_width - self.patch_w)\n                    y_start_actual = min(y_start, img_height - self.patch_h)\n\n                    x_start_actual = max(0, x_start_actual)\n                    y_start_actual = max(0, y_start_actual)\n\n                    self.patch_coords.append((img_idx, x_start_actual, y_start_actual))\n\n    def __len__(self):\n        return len(self.patch_coords)\n\n    def __getitem__(self, idx):\n        img_idx, x_start, y_start = self.patch_coords[idx]\n        aug_img_path, clean_img_path = self.data_pairs[img_idx]\n\n        aug_img = Image.open(aug_img_path).convert(\"RGB\")\n        clean_img = Image.open(clean_img_path).convert(\"RGB\")\n\n        aug_patch = aug_img.crop((x_start, y_start, x_start + self.patch_w, y_start + self.patch_h))\n        clean_patch = clean_img.crop((x_start, y_start, x_start + self.patch_w, y_start + self.patch_h))\n\n        if self.transform:\n            aug_patch = self.transform(aug_patch)\n            clean_patch = self.transform(clean_patch)\n\n        return aug_patch, clean_patch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T09:41:16.736355Z","iopub.execute_input":"2025-07-14T09:41:16.736873Z","iopub.status.idle":"2025-07-14T09:41:16.749082Z","shell.execute_reply.started":"2025-07-14T09:41:16.736851Z","shell.execute_reply":"2025-07-14T09:41:16.748320Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def predict_full_image_sliding_window(model, image_path, patch_size, overlap, transform, device):\n    model.eval()\n    input_image = Image.open(image_path).convert(\"RGB\")\n    img_width, img_height = input_image.size\n\n    patch_w, patch_h = patch_size\n    stride_w = patch_w - overlap\n    stride_h = patch_h - overlap\n\n    num_patches_x = max(1, math.ceil((img_width - patch_w + stride_w) / stride_w)) if img_width > patch_w else 1\n    num_patches_y = max(1, math.ceil((img_height - patch_h + stride_h) / stride_h)) if img_height > patch_h else 1\n\n    output_full_image = torch.zeros((1, 3, img_height, img_width), device=device, dtype=torch.float32)\n    overlap_counts = torch.zeros((1, 1, img_height, img_width), device=device, dtype=torch.float32)\n\n    with torch.no_grad():\n        for r_idx in range(num_patches_y):\n            for c_idx in range(num_patches_x):\n                x_start = c_idx * stride_w\n                y_start = r_idx * stride_h\n\n                x_start_actual = min(x_start, img_width - patch_w)\n                y_start_actual = min(y_start, img_height - patch_h)\n                \n                x_start_actual = max(0, x_start_actual)\n                y_start_actual = max(0, y_start_actual)\n\n                input_patch_pil = input_image.crop((x_start_actual, y_start_actual, x_start_actual + patch_w, y_start_actual + patch_h))\n                input_patch_tensor = transform(input_patch_pil).unsqueeze(0).to(device)\n\n                predicted_patch_tensor = model(input_patch_tensor)\n\n                output_full_image[:, :, y_start_actual : y_start_actual + patch_h, x_start_actual : x_start_actual + patch_w] += predicted_patch_tensor.squeeze(0)\n                overlap_counts[:, :, y_start_actual : y_start_actual + patch_h, x_start_actual : x_start_actual + patch_w] += 1\n\n    output_full_image = output_full_image / torch.clamp(overlap_counts, min=1)\n    return output_full_image.squeeze(0).cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T09:41:20.813769Z","iopub.execute_input":"2025-07-14T09:41:20.814068Z","iopub.status.idle":"2025-07-14T09:41:20.821390Z","shell.execute_reply.started":"2025-07-14T09:41:20.814045Z","shell.execute_reply":"2025-07-14T09:41:20.820829Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T09:41:24.595926Z","iopub.execute_input":"2025-07-14T09:41:24.596565Z","iopub.status.idle":"2025-07-14T09:41:24.599842Z","shell.execute_reply.started":"2025-07-14T09:41:24.596526Z","shell.execute_reply":"2025-07-14T09:41:24.599138Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    input_dir = \"/kaggle/input/augraphy-dataset/augraphy/input\"\n    aug_dirs = [\n        \"/kaggle/input/augraphy-dataset/augraphy/augraphy_output_1\",\n        \"/kaggle/input/augraphy-dataset/augraphy/augraphy_output_2\"\n    ]\n    PATCH_SIZE = (512, 512)\n    OVERLAP_SIZE = 96\n    NUM_IMAGES_FOR_TRAINING = -1\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_epochs = 70\n    batch_size = 8\n    learning_rate = 1e-4\n    use_data_parallel = torch.cuda.device_count() > 1\n    checkpoint_path = \"/kaggle/input/checkpoints-unet1/denoising_unet_patch_checkpoint.pth\"\n    start_epoch = 0\n\n    model = AttentionUNet(in_channels=3, out_channels=3)\n    if use_data_parallel:\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    criterion = nn.L1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    if os.path.exists(checkpoint_path):\n        try:\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n            if 'patch_size' in checkpoint and checkpoint['patch_size'] == PATCH_SIZE:\n                model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch'] + 1\n                print(f\"Resuming training from Epoch {start_epoch}\")\n            else:\n                print(f\"Checkpoint patch size {checkpoint.get('patch_size')} does not match current patch size {PATCH_SIZE}.\")\n                print(\"Starting training from scratch (Epoch 0).\")\n                start_epoch = 0\n        except Exception as e:\n            print(f\"Error loading checkpoint: {e}. Starting training from scratch (Epoch 0).\")\n            start_epoch = 0\n    else:\n        print(f\"No checkpoint found at {checkpoint_path}. Starting training from scratch (Epoch 0).\")\n        start_epoch = 0\n\n    dataset = AugraphyPatchDataset(\n        input_dir=input_dir,\n        aug_dirs=aug_dirs,\n        patch_size=PATCH_SIZE,\n        overlap=OVERLAP_SIZE,\n        transform=transform,\n        num_images_to_use=NUM_IMAGES_FOR_TRAINING\n    )\n\n    if len(dataset) == 0:\n        print(\"Dataset is empty. Cannot proceed with training. Please check paths and image pairing.\")\n    else:\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0, pin_memory=True)\n        \n        print(\"\\nStarting training...\")\n        for epoch in range(start_epoch, num_epochs):\n            model.train()\n            epoch_loss = 0.0\n            for batch_idx, (aug_patch, clean_patch) in enumerate(loader):\n                aug_patch, clean_patch = aug_patch.to(device), clean_patch.to(device)\n                optimizer.zero_grad()\n                output_patch = model(aug_patch)\n                loss = criterion(output_patch, clean_patch)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n                if batch_idx % 50 == 0:\n                    print(f\"  Epoch {epoch+1}, Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n            avg_epoch_loss = epoch_loss / len(loader)\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_epoch_loss:.4f}\")\n            print(f\"Saving checkpoint for Epoch {epoch+1}...\")\n\n            output_checkpoint_dir = \"/kaggle/working/checkpoints\"\n            os.makedirs(output_checkpoint_dir, exist_ok=True)\n            output_checkpoint_path = os.path.join(output_checkpoint_dir, \"denoising_unet_patch_checkpoint.pth\")\n\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_epoch_loss,\n                'patch_size': PATCH_SIZE,\n            }, output_checkpoint_path)\n            print(\"Checkpoint saved.\")\n        print(\"\\nTraining finished.\")\n        \n        if len(dataset.data_pairs) > 0:\n            random_idx = random.randint(0, len(dataset.data_pairs) - 1)\n            test_aug_image_path, _ = dataset.data_pairs[random_idx]\n\n            output_denoised_image_dir = \"/kaggle/working/denoised_output\"\n            os.makedirs(output_denoised_image_dir, exist_ok=True)\n            \n            output_file_name = f\"denoised_{os.path.basename(test_aug_image_path)}\"\n            output_denoised_image_path = os.path.join(output_denoised_image_dir, output_file_name)\n\n            print(f\"\\nPerforming full image inference on a randomly selected image: {test_aug_image_path}\")\n            \n            predicted_full_tensor = predict_full_image_sliding_window(\n                model=model,\n                image_path=test_aug_image_path,\n                patch_size=PATCH_SIZE,\n                overlap=OVERLAP_SIZE,\n                transform=transform,\n                device=device\n            )\n            \n            predicted_pil = transforms.ToPILImage()(predicted_full_tensor.clamp(0, 1))\n            predicted_pil.save(output_denoised_image_path)\n            print(f\"Denoised image saved to: {output_denoised_image_path}\")\n        else:\n            print(\"No image pairs found in the dataset for inference. Skipping inference.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:15:33.548784Z","iopub.execute_input":"2025-07-14T17:15:33.549297Z","iopub.status.idle":"2025-07-14T17:15:37.080781Z","shell.execute_reply.started":"2025-07-14T17:15:33.549278Z","shell.execute_reply":"2025-07-14T17:15:37.080004Z"}},"outputs":[{"name":"stdout","text":"Resuming training from Epoch 70\n\nStarting training...\n\nTraining finished.\n\nPerforming full image inference on a randomly selected image: /kaggle/input/augraphy-dataset/augraphy/augraphy_output_2/image_25_augmented.png\nDenoised image saved to: /kaggle/working/denoised_output/denoised_image_25_augmented.png\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import shutil\nshutil.copy(\"/kaggle/working/checkpoints/denoising_unet_patch_checkpoint.pth\", \"/kaggle/working/denoising_unet_patch_checkpoint.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:12:21.097863Z","iopub.execute_input":"2025-07-14T17:12:21.098644Z","iopub.status.idle":"2025-07-14T17:12:21.122342Z","shell.execute_reply.started":"2025-07-14T17:12:21.098605Z","shell.execute_reply":"2025-07-14T17:12:21.121815Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/denoising_unet_patch_checkpoint.pth'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}